{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('Sidecar/code/NLS/moving/PINNs_withR')\n",
    "\n",
    "gpu_list = [0, 1, 2, 3]\n",
    "cuda_device = ','.join([str(i) for i in gpu_list])\n",
    "\n",
    "print(f'Using GPU: {cuda_device}')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from scipy import integrate\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "from PINNs_para import parallel_train\n",
    "from parameters import params\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "num_processes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, \n",
    "            10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "param_vector = [50, 100, 200, 400]\n",
    "\n",
    "print(param_vector)\n",
    "\n",
    "result_matrix = np.zeros((len(param_vector), len(num_processes), 5))\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for width in tqdm(param_vector):\n",
    "    \n",
    "    shared_params = params()\n",
    "\n",
    "    shared_params.width = width\n",
    "    shared_params.epoches = 100000\n",
    "    shared_params.n_inner_points = 128\n",
    "    shared_params.n_bic_points = 128\n",
    "    shared_params.x_l = -15.0\n",
    "    shared_params.x_r = 15.0\n",
    "    shared_params.batch_size = 2046\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "\n",
    "        manager = mp.Manager()\n",
    "        results_queue = manager.Queue()\n",
    "\n",
    "        num_tasks = num_processes\n",
    "        num_gpus = len(gpu_list)\n",
    "\n",
    "        processes = []\n",
    "\n",
    "        for task_id in num_tasks:\n",
    "            \n",
    "            gpu_id = task_id % num_gpus\n",
    "\n",
    "            print(f\"Task {task_id} on GPU {gpu_list[gpu_id]}\")\n",
    "                \n",
    "            p = mp.Process(target=parallel_train, args=(task_id, shared_params, results_queue, gpu_id))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "    \n",
    "\n",
    "        while not results_queue.empty():\n",
    "            (idx, loss_vector) = results_queue.get()\n",
    "            result_matrix[i, idx, :] = loss_vector\n",
    "            \n",
    "    i += 1\n",
    "\n",
    "print(result_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
