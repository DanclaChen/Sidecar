{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/PINNs')\n",
    "\n",
    "cuda_device = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:8'\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from scipy import integrate\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "from PINNs import parallel_train\n",
    "from parameters import params\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.set_default_device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "num_processes = 5\n",
    "\n",
    "width_R = 4\n",
    "\n",
    "param_vector = [4, 8, 16, 32, 64]\n",
    "\n",
    "param_vector = [width_R + i for i in param_vector]\n",
    "\n",
    "print(param_vector)\n",
    "\n",
    "result_matrix = np.zeros((len(param_vector), num_processes, 4))\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "for width in tqdm(param_vector):\n",
    "    \n",
    "    shared_params = params()\n",
    "\n",
    "    shared_params.width = width\n",
    "    shared_params.h_train = 2/256\n",
    "    shared_params.num_hidden = 2\n",
    "    shared_params.epoches = 30000\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "            \n",
    "        mp.set_start_method('spawn', force=True)\n",
    "        \n",
    "        manager = mp.Manager()\n",
    "        results_queue = manager.Queue()\n",
    "\n",
    "        mp.spawn(parallel_train, args=(shared_params, results_queue), nprocs=num_processes, join=True)\n",
    "        \n",
    "        # print(results_queue.get())\n",
    "\n",
    "        while not results_queue.empty():\n",
    "            (idx, loss_vector) = results_queue.get()\n",
    "    #             print(f\"Index: {idx}, error: {error}, error_infty: {error_infty}\")\n",
    "            result_matrix[i, idx, :] = loss_vector\n",
    "            \n",
    "    i += 1\n",
    "\n",
    "print(result_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
